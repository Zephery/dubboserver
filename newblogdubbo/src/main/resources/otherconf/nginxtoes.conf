input {
    file {
      type => "nginx_access"
        path => "/usr/share/nginx/logs/access.log"
        codec => "json"
    }
}

filter {
  if [type] == "nginx_access" {

    geoip {
      source => "ip"
      target => "geoip"
      #database => "/etc/logstash/GeoLiteCity.dat"  ##可要可不要
      add_field => [ "[geoip][coordinates]", "%{[geoip][longitude]}" ]
      add_field => [ "[geoip][coordinates]", "%{[geoip][latitude]}"  ]
    }
    mutate {
      convert => [ "[geoip][coordinates]", "float"]
    }
  }
}
output {
  stdout { codec => rubydebug }
  elasticsearch {
        hosts => ["119.29.188.224:9200"]
        index => "logstash-%{type}-%{+YYYY.MM.dd}"
        document_type => "%{type}"
        ##flush_size => 20000
        ##idle_flush_time => 10
        #sniffing => true
        #template_overwrite => true
    }
 kafka {
    zk_connect => "119.29.188.224:2181"    #zookeeper地址
    topic_id => "nginx-access-log"    #kafka中topic名称，记得创建该topic
    group_id => "nginx-access-log"     #默认为“logstash”
    codec => "plain"    #与Shipper端output配置项一致
    consumer_threads => 1    #消费的线程数
    decorate_events => true    #在输出消息的时候回输出自身的信息，包括：消费消息的大小、topic来源以及consumer的group信息。
    type => "nginx-access-log"
  }
}
